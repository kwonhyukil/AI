# ğŸ’¡ [í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°]
from sklearn.datasets import load_digits              # ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (0~9)
from sklearn.model_selection import train_test_split  # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜
from sklearn.preprocessing import StandardScaler      # í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ í‘œì¤€í™”
import numpy as np                                     # ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬

np.set_printoptions(suppress=True)  # ì§€ìˆ˜ í‘œê¸°(e.g. 1e-5) ìƒëµí•˜ê³  ë³´ê¸° ì‰½ê²Œ ì¶œë ¥

# ğŸ“Œ 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
dataset = load_digits()     # ì†ê¸€ì”¨ ì´ë¯¸ì§€ (64ì°¨ì› ë²¡í„°), 10ê°œ í´ë˜ìŠ¤ (0~9)
X = dataset.data            # ì…ë ¥ íŠ¹ì„± (8x8 ì´ë¯¸ì§€ â†’ 64ì°¨ì› ë²¡í„°)
y = dataset.target          # ì •ë‹µ ë ˆì´ë¸” (0~9)

# ğŸ“Œ 2. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
# stratify=y: í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê° í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ë™ì¼í•˜ê²Œ ìœ ì§€

# ğŸ“Œ 3. íŠ¹ì„± ê°’ í‘œì¤€í™” (Standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # í‰ê·  0, ë¶„ì‚° 1ë¡œ ë³€í™˜ (í›ˆë ¨ ê¸°ì¤€)
X_test = scaler.transform(X_test)        # ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë³€í™˜

# ğŸ“Œ 4. í•™ìŠµ ì„¤ì •
num_features = X_train.shape[1]  # íŠ¹ì„± ê°œìˆ˜ = 64 (8x8 ì´ë¯¸ì§€)
num_samples = X_train.shape[0]   # í›ˆë ¨ ìƒ˜í”Œ ìˆ˜
num_classes = 10                 # ìˆ«ì í´ë˜ìŠ¤ (0~9)

# ğŸ“Œ 5. ì •ë‹µ ë ˆì´ë¸”ì„ One-hot ì¸ì½”ë”©
one_hot = np.eye(num_classes)        # ë‹¨ìœ„í–‰ë ¬ (10x10) â†’ ì˜ˆ: 3 â†’ [0 0 0 1 0 0 0 0 0 0]
y_train_one_hot = one_hot[y_train]   # í›ˆë ¨ ë°ì´í„°ìš© one-hot ë ˆì´ë¸”
y_test_one_hot = one_hot[y_test]     # í…ŒìŠ¤íŠ¸ ë°ì´í„°ìš© one-hot ë ˆì´ë¸”

# ğŸ“Œ 6. ê°€ì¤‘ì¹˜(w), í¸í–¥(b) ì´ˆê¸°í™” (ì •ê·œë¶„í¬ ê¸°ë°˜)
learning_rate = 0.01
epochs = 300
w = np.random.randn(num_features, num_classes)  # (64, 10): ê° íŠ¹ì„±ë³„ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
b = np.random.randn(num_classes)                # (10,): ê° í´ë˜ìŠ¤ë³„ í¸í–¥

# ğŸ“Œ 7. í•™ìŠµ ì‹œì‘ (Softmax íšŒê·€ ë°˜ë³µ í•™ìŠµ)
for epoch in range(epochs):

    # 7-1. ì„ í˜•ê²°í•© (logits ê³„ì‚°): z = Xw + b
    logits = X_train @ w + b                   # shape: (ìƒ˜í”Œ ìˆ˜, í´ë˜ìŠ¤ ìˆ˜)
    logits -= logits.max(axis=1, keepdims=True)  # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ (softmax ì•ˆì •ì„± ì²˜ë¦¬)

    # 7-2. Softmax í•¨ìˆ˜ ì ìš© (í™•ë¥ ë¡œ ë³€í™˜)
    exp_logits = np.exp(logits)                # ì§€ìˆ˜ ê³„ì‚°: e^z
    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)  # í´ë˜ìŠ¤ë³„ í•©
    softmax = exp_logits / sum_exp             # í™•ë¥  ë¶„í¬ë¡œ ì •ê·œí™”

    # 7-3. ì˜¤ì°¨(error) ê³„ì‚°: ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’
    error = softmax - y_train_one_hot          # shape: (ìƒ˜í”Œ ìˆ˜, í´ë˜ìŠ¤ ìˆ˜)

    # 7-4. ê¸°ìš¸ê¸° ê³„ì‚° (Gradient ê³„ì‚°)
    gradient_w = X_train.T @ error / num_samples  # ê°€ì¤‘ì¹˜ wì— ëŒ€í•œ í‰ê·  ê¸°ìš¸ê¸°
    gradient_b = error.mean(axis=0)               # í¸í–¥ bì— ëŒ€í•œ í‰ê·  ê¸°ìš¸ê¸°

    # 7-5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (Gradient Descent)
    w -= learning_rate * gradient_w             # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    b -= learning_rate * gradient_b             # í¸í–¥ ì—…ë°ì´íŠ¸

    # 7-6. ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° (Cross Entropy Loss)
    loss = -np.sum(y_train_one_hot * np.log(softmax + 1e-15)) / num_samples
    # log(softmax) ê°’ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ 1e-15 ë”í•¨ (log 0 ë°©ì§€)

    # 7-7. ì¤‘ê°„ ì¶œë ¥ (100 epochë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥)
    if epoch % 100 == 0:
        print(f'[Epoch {epoch}] Loss: {loss:.4f}')

# ğŸ“Œ 8. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
test_logits = X_test @ w + b
test_logits -= test_logits.max(axis=1, keepdims=True)  # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
exp_test_logits = np.exp(test_logits)
sum_exp_test = np.sum(exp_test_logits, axis=1, keepdims=True)
softmax_test = exp_test_logits / sum_exp_test          # softmax ê²°ê³¼ (í™•ë¥  ë¶„í¬)

# ğŸ“Œ 9. í…ŒìŠ¤íŠ¸ ì†ì‹¤ ê³„ì‚°
test_loss = -np.sum(y_test_one_hot * np.log(softmax_test + 1e-15)) / y_test_one_hot.shape[0]
print(f'\n[Test Loss] {test_loss:.4f}')

# ğŸ“Œ 10. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê³„ì‚°
y_pred = np.argmax(softmax_test, axis=1)       # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ
accuracy = np.mean(y_pred == y_test)           # ì •ë‹µê³¼ ì¼ì¹˜í•œ ë¹„ìœ¨
print(f'[Test Accuracy] {accuracy:.4f} ({accuracy * 100:.2f}%)')
